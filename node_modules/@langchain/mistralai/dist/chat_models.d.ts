/// <reference path="../src/mistralai.d.ts" />
import { type BaseMessage } from "@langchain/core/messages";
import { type BaseLanguageModelCallOptions } from "@langchain/core/language_models/base";
import { type ChatCompletionResult as MistralAIChatCompletionResult, type ChatCompletionOptions as MistralAIChatCompletionOptions } from "@mistralai/mistralai";
import { CallbackManagerForLLMRun } from "@langchain/core/callbacks/manager";
import { type BaseChatModelParams, BaseChatModel } from "@langchain/core/language_models/chat_models";
import { ChatGenerationChunk, ChatResult } from "@langchain/core/outputs";
/**
 * Input to chat model class.
 */
export interface ChatMistralAIInput extends BaseChatModelParams {
    /**
     * The API key to use.
     * @default {process.env.MISTRAL_API_KEY}
     */
    apiKey?: string;
    /**
     * The name of the model to use.
     * @default {"mistral-small"}
     */
    modelName?: string;
    /**
     * Override the default endpoint.
     */
    endpoint?: string;
    /**
     * What sampling temperature to use, between 0.0 and 2.0.
     * Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
     * @default {0.7}
     */
    temperature?: number;
    /**
     * Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass.
     * So 0.1 means only the tokens comprising the top 10% probability mass are considered.
     * Should be between 0 and 1.
     * @default {1}
     */
    topP?: number;
    /**
     * The maximum number of tokens to generate in the completion.
     * The token count of your prompt plus max_tokens cannot exceed the model's context length.
     */
    maxTokens?: number;
    /**
     * Whether or not to stream the response.
     * @default {false}
     */
    streaming?: boolean;
    /**
     * Whether to inject a safety prompt before all conversations.
     * @default {false}
     */
    safeMode?: boolean;
    /**
     * The seed to use for random sampling. If set, different calls will generate deterministic results.
     */
    randomSeed?: number;
}
/**
 * Integration with a chat model.
 */
export declare class ChatMistralAI<CallOptions extends BaseLanguageModelCallOptions = BaseLanguageModelCallOptions> extends BaseChatModel<CallOptions> implements ChatMistralAIInput {
    static lc_name(): string;
    modelName: string;
    apiKey: string;
    endpoint?: string;
    temperature: number;
    streaming: boolean;
    topP: number;
    maxTokens: number;
    safeMode: boolean;
    randomSeed?: number;
    lc_serializable: boolean;
    constructor(fields?: ChatMistralAIInput);
    _llmType(): string;
    /**
     * Get the parameters used to invoke the model
     */
    invocationParams(): Omit<MistralAIChatCompletionOptions, "messages">;
    /**
     * Calls the MistralAI API with retry logic in case of failures.
     * @param {MistralAIChatCompletionOptions} input The input to send to the MistralAI API.
     * @returns {Promise<MistralAIChatCompletionResult | AsyncGenerator<MistralAIChatCompletionResult>>} The response from the MistralAI API.
     */
    completionWithRetry(input: MistralAIChatCompletionOptions, streaming: true): Promise<AsyncGenerator<MistralAIChatCompletionResult>>;
    completionWithRetry(input: MistralAIChatCompletionOptions, streaming: false): Promise<MistralAIChatCompletionResult>;
    /** @ignore */
    _generate(messages: BaseMessage[], options: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): Promise<ChatResult>;
    _streamResponseChunks(messages: BaseMessage[], options: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<ChatGenerationChunk>;
    /** @ignore */
    _combineLLMOutput(): never[];
    imports(): Promise<{
        MistralClient: typeof import("@mistralai/mistralai").default;
    }>;
}
