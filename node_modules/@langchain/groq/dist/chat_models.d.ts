import { CallbackManagerForLLMRun } from "@langchain/core/callbacks/manager";
import { BaseChatModel, BaseChatModelCallOptions, type BaseChatModelParams } from "@langchain/core/language_models/chat_models";
import { BaseMessage } from "@langchain/core/messages";
import { ChatGenerationChunk, ChatResult } from "@langchain/core/outputs";
import { type OpenAICoreRequestOptions } from "@langchain/openai";
import Groq from "groq-sdk";
import { ChatCompletionChunk } from "groq-sdk/lib/chat_completions_ext";
import { ChatCompletion, ChatCompletionCreateParams, ChatCompletionCreateParamsNonStreaming, ChatCompletionCreateParamsStreaming } from "groq-sdk/resources/chat/completions";
export interface ChatGroqCallOptions extends BaseChatModelCallOptions {
}
export interface ChatGroqInput extends BaseChatModelParams {
    /**
     * The Groq API key to use for requests.
     * @default process.env.GROQ_API_KEY
     */
    apiKey?: string;
    /**
     * The name of the model to use.
     * @default "llama2-70b-4096"
     */
    modelName?: string;
    /**
     * Up to 4 sequences where the API will stop generating further tokens. The
     * returned text will not contain the stop sequence.
     */
    stop?: string | null | Array<string>;
    /**
     * Whether or not to stream responses.
     */
    streaming?: boolean;
    /**
     * The temperature to use for sampling.
     * @default 0.7
     */
    temperature?: number;
}
type GroqRoleEnum = "system" | "assistant" | "user" | "function";
export declare function messageToGroqRole(message: BaseMessage): GroqRoleEnum;
/**
 * Wrapper around Groq API for large language models fine-tuned for chat
 *
 * Groq API is compatible to the OpenAI API with some limitations. View the
 * full API ref at:
 * @link {https://docs.api.groq.com/md/openai.oas.html}
 *
 * To use, you should have the `GROQ_API_KEY` environment variable set.
 * @example
 * ```typescript
 * const model = new ChatGroq({
 *   temperature: 0.9,
 *   apiKey: process.env.GROQ_API_KEY,
 * });
 *
 * const response = await model.invoke([new HumanMessage("Hello there!")]);
 * console.log(response);
 * ```
 */
export declare class ChatGroq extends BaseChatModel<ChatGroqCallOptions> {
    client: Groq;
    modelName: string;
    temperature: number;
    streaming: boolean;
    static lc_name(): string;
    _llmType(): string;
    get lc_secrets(): {
        [key: string]: string;
    } | undefined;
    lc_serializable: boolean;
    constructor(fields?: ChatGroqInput);
    completionWithRetry(request: ChatCompletionCreateParamsStreaming, options?: OpenAICoreRequestOptions): Promise<AsyncIterable<ChatCompletionChunk>>;
    completionWithRetry(request: ChatCompletionCreateParamsNonStreaming, options?: OpenAICoreRequestOptions): Promise<ChatCompletion>;
    invocationParams(options: this["ParsedCallOptions"]): ChatCompletionCreateParams;
    _streamResponseChunks(messages: BaseMessage[], options: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<ChatGenerationChunk>;
    _generate(messages: BaseMessage[], options: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): Promise<ChatResult>;
}
export {};
